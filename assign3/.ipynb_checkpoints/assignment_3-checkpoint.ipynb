{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. You have to use only this notebook for all your code.\n",
    "2. All the results and plots should be mentioned in this notebook.\n",
    "3. For final submission, submit this notebook along with the report ( usual 2-4 pages, latex typeset, which includes the challenges faces and details of additional steps, if any)\n",
    "4. Marking scheme\n",
    "    -  **60%**: Your code should be able to detect bounding boxes using resnet 18, correct data loading and preprocessing. Plot any 5 correct and 5 incorrect sample detections from the test set in this notebook for both the approached (1 layer and 2 layer detection), so total of 20 plots.\n",
    "    -  **20%**: Use two layers (multi-scale feature maps) to detect objects independently as in SSD (https://arxiv.org/abs/1512.02325).  In this method, 1st detection will be through the last layer of Resnet18 and the 2nd detection could be through any layer before the last layer. SSD uses lower resolution layers to detect larger scale objects. \n",
    "    -  **20%**: Implement Non-maximum suppression (NMS) (should not be imported from any library) on the candidate bounding boxes.\n",
    "    \n",
    "5. Report AP for each of the three class and mAP score for the complete test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, i.e. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the three classes(aeroplane, bottle, chair). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be four. This is important for applying the sliding window method later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "import os, random , pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Import other modules if required\n",
    "# Can use other libraries as well\n",
    "\n",
    "resnet_input = [224, 224, 3]\n",
    "c_dir = os.getcwd()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane',\n",
    "           'bottle',\n",
    "           'chair'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class voc_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        self.train = train\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if(train is True):\n",
    "            dict_file = os.path.join(self.root_dir, \"train.pkl\")\n",
    "            filehandler = open(dict_file,\"rb\")\n",
    "            self.dict = pickle.load(filehandler)\n",
    "        else:\n",
    "            dict_file = os.path.join(self.root_dir, \"test.pkl\")\n",
    "            filehandler = open(dict_file,\"rb\")\n",
    "            self.dict = pickle.load(filehandler)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train is True:\n",
    "            img_f = os.path.join(self.root_dir, \"train\")\n",
    "        else:\n",
    "            img_f = os.path.join(self.root_dir, \"test\")\n",
    "            \n",
    "        img_name = os.path.join(img_f, \"img\" + str(idx) + \".jpg\")\n",
    "#        img = io.imread(img_name)\n",
    "      \n",
    "        img = io.imread(img_name)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.dict[idx] == \"__background__\":\n",
    "            label = [1,0,0,0]\n",
    "        if self.dict[idx] == \"aeroplane\":\n",
    "            label = [0,1,0,0]\n",
    "        if self.dict[idx] == \"bottle\":\n",
    "            label = [0,0,1,0]\n",
    "        if self.dict[idx] == \"chair\":\n",
    "            label = [0,0,0,1]\n",
    "            \n",
    "        return img,  np.array(label)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    \"\"\"\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def sliding_window(image_s, stepSize, windowSize):\n",
    "    ''' Helper Sliding window function '''\n",
    "    for y in range(0, image_s[0], stepSize):\n",
    "        for x in range(0, image_s[1], stepSize):\n",
    "            yield (x, y, (windowSize[0] ,  windowSize[1]) )\n",
    "            \n",
    "def background(img, patches):\n",
    "    ''' Generate background with iou < 0.2 using sliding window'''\n",
    "    aspect_ratios = [(400, 400), (224,224), (120, 300) ,(250,100)]\n",
    "    candidates = []\n",
    "    for each in aspect_ratios:\n",
    "        (winH, winW) = each\n",
    "        for (x, y, window_s) in sliding_window(img.shape, stepSize=min(winH,winW), windowSize=(winH, winW)):\n",
    "            if window_s[0] != winH or window_s[1] != winW:\n",
    "                    continue\n",
    "            candidate = True\n",
    "            for each in patches:\n",
    "                iou = get_iou((x, y , x + window_s[1], y + window_s[0]), each)\n",
    "                if(iou > 0.2):\n",
    "                    candidate = False\n",
    "            if candidate:\n",
    "                candidates.append((x, y , x + window_s[1], y + window_s[0]))\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def build_dataset(typ = \"train\"):\n",
    "    ''' Build the dataset for classifier training and testing '''\n",
    "    print(\"Building dataset from PASCAL VOC DATASET\")\n",
    "    train_img_addr = c_dir + \"/\" + \"VOC_\" + typ + \"/JPEGImages\"\n",
    "    train_ann_addr = c_dir + \"/\" + \"VOC_\" + typ + \"/Annotations\"\n",
    "    train_images = os.listdir(train_img_addr)\n",
    "    train_id = 0\n",
    "    train_dict = {}\n",
    "    g_take_back = 0\n",
    "    count = [0,0,0,0,0]\n",
    "    for each in train_images:\n",
    "        tree =  ET.parse( train_ann_addr + '/' + each[:-3] + 'xml')\n",
    "        root = tree.getroot()\n",
    "        objects = []\n",
    "        take_back = 0\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            box = obj.find('bndbox')\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "\n",
    "            img = io.imread(train_img_addr + '/' + each)\n",
    "            if name in classes:\n",
    "                objects.append((xmin, ymin, xmax, ymax))\n",
    "                take_back =  1\n",
    "                c_img = img[ ymin:ymax, xmin:xmax]\n",
    "                r_img = resize(c_img, (resnet_input[0], resnet_input[1]))\n",
    "                io.imsave(c_dir + \"/data/\" + typ + \"/img\" + str(train_id) + \".jpg\",r_img)\n",
    "                train_dict[train_id] = name\n",
    "                train_id = train_id + 1\n",
    "                print(train_id)\n",
    "                if name == \"aeroplane\":\n",
    "                    count[0] = count[0] + 1\n",
    "                if name == \"bottle\":\n",
    "                    count[1] = count[1] + 1\n",
    "                if name == \"chair\":\n",
    "                    count[2] = count[2] + 1\n",
    "                            \n",
    "        g_take_back = g_take_back + 1\n",
    "        \n",
    "        if take_back == 1:\n",
    "            b_img = background(img, objects)\n",
    "            r_img = resize(b_img, (resnet_input[0], resnet_input[1]))\n",
    "            io.imsave(c_dir + \"/data/\" + typ + \"/img\" + str(train_id) + \".jpg\",r_img)\n",
    "            io.imsave(c_dir + \"/data/\" + \"back\" + \"/img\" + str(train_id) + \".jpg\",r_img)\n",
    "            train_dict[train_id] = '__background__'\n",
    "            train_id = train_id + 1\n",
    "            count[3] = count[3] + 1\n",
    "            print(\"back\", train_id)\n",
    "            \n",
    "        elif g_take_back % 4 == 0:\n",
    "            b_img = background(img, objects)\n",
    "            r_img = resize(b_img, (resnet_input[0], resnet_input[1]))\n",
    "            io.imsave(c_dir + \"/data/\" + typ + \"/img\" + str(train_id) + \".jpg\",r_img)\n",
    "            io.imsave(c_dir + \"/data/\" + \"back\" + \"/img\" + str(train_id) + \".jpg\",r_img)\n",
    "\n",
    "            train_dict[train_id] = '__background__'\n",
    "            train_id = train_id + 1\n",
    "            count[4] = count[4] + 1\n",
    "            print(\"back\", train_id)\n",
    "    \n",
    "    filehandler = open(\"data/\" + typ +\".pkl\",\"wb\")\n",
    "    pickle.dump(train_dict,filehandler)\n",
    "    return count\n",
    "            \n",
    "count1 = build_dataset(\"train\")\n",
    "count2 = build_dataset(\"test\")\n",
    "print(count1,count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The networks\n",
    "<br/> Training the network on the created dataset. This will yield a classification network on the 4 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One layer network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one layer network using pretrained resnet\n",
    "def resnetOneLayer():\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 4)    \n",
    "    ct = 0 \n",
    "    for child in resnet18.children():\n",
    "        ct += 1\n",
    "        if ct < 8:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    return resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two layer network using pretrained resnet\n",
    "class resnetTwoLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        children = list(resnet18.children())\n",
    "        # features upto 2nd last layer\n",
    "        self.features = torch.nn.Sequential(*(children[:-3]))\n",
    "\n",
    "        # Freeze the layers upto above\n",
    "        for layer in self.features.children():\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # The last ResNet block\n",
    "        self.last = children[-3]\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # caoncat the last and 2nd last layer, 256, 512 size\n",
    "        self.fc = torch.nn.Sequential(torch.nn.Linear(256 + 512, 4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        y = self.last(x)  \n",
    "        x = self.avgpool(x)\n",
    "        y = self.avgpool(y)\n",
    "        # Concatenate before and after\n",
    "        x = torch.squeeze(torch.squeeze(torch.cat([x, y], dim=1), dim=3), dim=2)\n",
    "        return self.fc(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and testing function\n",
    "Using the newly made pre-trained network to fine-tune the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, batch_size):\n",
    "    c_dir = os.getcwd()   \n",
    "    composed_transform = transforms.Compose([ \n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(224, 224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    train_dataset = voc_dataset(root_dir= c_dir + '/data', train=True, transform=composed_transform) \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):  \n",
    "        running_loss = 0.0\n",
    "        accuracy_sum = 0.0\n",
    "        t_loss = [0,0]\n",
    "        t_accur = [0,0]\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "             #Accuracy\n",
    "            output = torch.max(outputs, 1)[1]\n",
    "            label = torch.max(labels, 1)[1]\n",
    "            correct = (output == label).float().sum()\n",
    "            accr = correct/output.shape[0] \n",
    "            accuracy_sum = accuracy_sum + accr\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f accurcy: %.3f'  %\n",
    "                      (epoch + 1, i + 1, running_loss / 20, accuracy_sum/ 20 ))\n",
    "                t_loss[0] = t_loss[0] + running_loss\n",
    "                t_loss[1] = t_loss[1] +  20\n",
    "                t_accur[0] = t_accur[0] + accuracy_sum\n",
    "                t_accur[1] =  t_accur[1] + 20\n",
    "                running_loss = 0.0\n",
    "                accuracy_sum = 0.0\n",
    "                \n",
    "        print(\"EPOCH SUMMARY: loss \", t_loss[0]/t_loss[1], \" Accuracy \" , t_accur[0]/t_accur[1] )\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "def test_accuracy(model, model_state, batch_size):\n",
    "    c_dir = os.getcwd()   \n",
    "    composed_transform = transforms.Compose([ \n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(224, 224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    test_dataset = voc_dataset(root_dir= c_dir + '/data', train=False, transform=composed_transform) \n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_state))\n",
    "    model.eval()\n",
    "    \n",
    "    ## Test accuarcy overall    \n",
    "    ## classwiz=se accuracy\n",
    "    class_correct = list(0. for i in range(4))\n",
    "    class_total = list(0. for i in range(4))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            label = torch.max(labels, 1)[1]\n",
    "    \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "            c = (predicted == label)\n",
    "            for i in range(c.size(0)):\n",
    "                class_correct[label[i]] += int(c[i].item())\n",
    "                class_total[label[i]] += 1\n",
    "                \n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "    for i in range(4):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            i , 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Layer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Layer Detection\n",
    "def train_and_test_one_leyer_model():\n",
    "    ### One LAYER MODEL TRAIN AND TEST\n",
    "    model = resnetOneLayer().to(device)\n",
    "    saved_name = \"./model/one_layer_t.pt\"\n",
    "    \n",
    "    num_epochs = 10\n",
    "    learning_rate =  0.001\n",
    "    hyp_momentum = 0.9\n",
    "    batch_size = 30\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate, hyp_momentum)\n",
    "    \n",
    "    # Find total parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"total_params:\",total_params)\n",
    "    total_trainable_params = sum( p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"total_trainable_params:\" ,  total_trainable_params)\n",
    "    \n",
    "    train(model, criterion, optimizer, num_epochs, batch_size)\n",
    "    torch.save(model.state_dict(), saved_name)\n",
    "    \n",
    "    test_accuracy(model,saved_name, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Layer Detection (SSD)\n",
    "def train_and_test_two_layer():\n",
    "    ### TWO LAYER MODEL TRAIN AND TEST\n",
    "    model = resnetOneLayer().to(device)\n",
    "    saved_name = \"./model/two_layer_t.pt\"\n",
    "    \n",
    "    num_epochs = 10\n",
    "    learning_rate =  0.001\n",
    "    hyp_momentum = 0.9\n",
    "    batch_size = 30\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate, hyp_momentum)\n",
    "    \n",
    "    # Find total parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"total_params:\",total_params)\n",
    "    total_trainable_params = sum( p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"total_trainable_params:\" ,  total_trainable_params)\n",
    "    \n",
    "    train(model, criterion, optimizer, num_epochs, batch_size)\n",
    "    torch.save(model.state_dict(), saved_name)\n",
    "    \n",
    "    test_accuracy(model,saved_name, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pyramid(image, scale, minSize=(60, 60)):\n",
    "    yield image\n",
    "    # keep looping over the pyramid\n",
    "    while True:\n",
    "        # compute the new dimensions of the image and resize it\n",
    "        h = int(image.shape[0] / scale)\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = resize(image, (h,w))\n",
    "        \n",
    "        image = image * 255   \n",
    "        image = image.astype('uint8')         \n",
    "        # if the resized image does not meet the supplied minimum\n",
    "        # size, then stop constructing the pyramid\n",
    "        if image.shape[0] < minSize[0] or image.shape[1] < minSize[1]:\n",
    "            break\n",
    "\n",
    "        # yield the next image in the pyramid\n",
    "        yield image\n",
    "\n",
    "def sliding_window(image, stepSize, windowSize):\n",
    "    # slide a window across the image\n",
    "    for y in range(0, image.shape[0], stepSize):\n",
    "        for x in range(0, image.shape[1], stepSize):\n",
    "            # yield the current window\n",
    "    yield (x, y, image[y:y + windowSize[0], x:x + windowSize[1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_suppression(dets, thresh):\n",
    "    if len(dets) == 0:\n",
    "        return []\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "        \n",
    "    return dets[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(xml_file):\n",
    "    tree =  ET.parse( xml_file)\n",
    "    root = tree.getroot()\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    actual_boxes = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        box = obj.find('bndbox')\n",
    "        xmin = int(box.find('xmin').text)\n",
    "        ymin = int(box.find('ymin').text)\n",
    "        xmax = int(box.find('xmax').text)\n",
    "        ymax = int(box.find('ymax').text)\n",
    "        \n",
    "        if name == 'aeroplane':\n",
    "            boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]))\n",
    "            labels.append(1.0)\n",
    "            difficulties.append(0.0)\n",
    "            actual_boxes.append([xmin, ymin, xmax, ymax, 1])\n",
    "        if name == 'bottle':\n",
    "            boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]))\n",
    "            labels.append(2.0)\n",
    "            difficulties.append(0.0)\n",
    "            actual_boxes.append([xmin, ymin, xmax, ymax, 2])\n",
    "\n",
    "        if name == 'chair':\n",
    "            boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]))\n",
    "            labels.append(3.0)\n",
    "            difficulties.append(0.0)\n",
    "            actual_boxes.append([xmin, ymin, xmax, ymax, 3])\n",
    "            \n",
    "    r_boxes = torch.tensor(boxes) if len(boxes) == 0 else torch.stack(boxes)\n",
    "    return actual_boxes, r_boxes, torch.tensor(labels), torch.tensor(difficulties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_bounding_box(model, img_name, actual_box):\n",
    "    '''This takes a test image and a model(either one layer or two layer) and gives the predicted bounding \n",
    "       box after doing sliding window, predicting the class and applying nms.\n",
    "    '''\n",
    "    image = io.imread(img_name)\n",
    "    count = 0    \n",
    "    boxes = []\n",
    "    \n",
    "    image_batch = []\n",
    "    image_shape = []\n",
    "    \n",
    "    composed_transform = transforms.Compose([ \n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    img_t = composed_transform(image)\n",
    "    image_batch.append(img_t)\n",
    "    image_shape.append((10, 10, (image.shape[1]) - 10, (image.shape[0]) - 10))\n",
    "  \n",
    "    for resized in pyramid(image, scale=1.8):\n",
    "        to_mult = image.shape[0]/ resized.shape[0]\n",
    "        aspect_ratios = [(200, 200), (96, 256), (156,96)]\n",
    "        for each in aspect_ratios:\n",
    "            count += 1\n",
    "            (winH, winW) = each\n",
    "            for (x, y, window) in sliding_window(resized, stepSize=50, windowSize=(winH, winW)):\n",
    "                if window.shape[0] != winH or window.shape[1] != winW:\n",
    "                    continue\n",
    "                img_t = composed_transform(window)\n",
    "                xmin = x*to_mult\n",
    "                ymin = y*to_mult\n",
    "                xmax = (x + window.shape[1]) *to_mult\n",
    "                ymax = (y + window.shape[0]) *to_mult\n",
    "                image_batch.append(img_t)\n",
    "                image_shape.append((xmin, ymin, xmax, ymax))\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        inputs = torch.stack(image_batch).to(device) if len(image_batch) != 0 else torch.tensor(image_batch).to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.to(device)\n",
    "        predicted = torch.max(outputs, 1)[1]\n",
    "        for i, each in enumerate(image_shape):\n",
    "            xmin, ymin, xmax, ymax = each\n",
    "            label = predicted[i].item()\n",
    "            prob = F.softmax(outputs[i], dim=0)\n",
    "            score = prob[label].item()\n",
    "            if label == 0:\n",
    "                continue\n",
    "            if score < 0.7 and (label == 2 or label == 3):\n",
    "                continue\n",
    "            if label == 2 and (ymax-ymin) < (xmax-xmin):\n",
    "                continue\n",
    "            if label == 3 and (1.1 * (ymax-ymin)) < (xmax-xmin):\n",
    "                continue\n",
    "            boxes.append( (xmin, ymin, xmax, ymax, score, label) )\n",
    "            \n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    pick = non_maximum_suppression(boxes, 0.1)\n",
    "#    print(\"[x] before applying non-maximum, %d bounding boxes\" % (boxes.shape[0]) )\n",
    "#    print(\"[x] after applying non-maximum, %d bounding boxes\" % (len(pick)) )    \n",
    "    \n",
    "#\n",
    "#    img = image.copy()\n",
    "#    for (startX, startY, endX, endY, cfd, lbl) in pick:\n",
    "#        cv2.rectangle(img, (int(startX), int(startY)), (int(endX), int(endY)), (0, 255, 0), 2)\n",
    "#        cv2.putText(img, str(lbl) ,(int(startX), int(startY)+ 20 ), cv2.FONT_HERSHEY_SIMPLEX, 1 ,(0,255,0), 2)\n",
    "#        cv2.putText(img, str(int(cfd*100)) ,(int(endX) - 20, int(endY) - 20 ), cv2.FONT_HERSHEY_SIMPLEX, 1 ,(0,255,0), 2)\n",
    "#    for (startX, startY, endX, endY, lbl) in actual_box:\n",
    "#        cv2.rectangle(img, (int(startX), int(startY)), (int(endX), int(endY)), (255, 255, 0), 2)\n",
    "#        cv2.putText(img, str(lbl) ,(int(startX), int(startY) + 20), cv2.FONT_HERSHEY_SIMPLEX, 1 ,(150,255,150), 2)\n",
    "#        \n",
    "#    plt.imshow(img)\n",
    "#    plt.show()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    scores = list()\n",
    "    for xmin , ymin, xmax , ymax , score , label  in pick:\n",
    "        boxes.append(torch.tensor([xmin, ymin, xmax, ymax]))\n",
    "        labels.append(float(label))\n",
    "        scores.append(score)\n",
    "    \n",
    "    r_boxes = torch.tensor(boxes) if len(boxes) == 0 else torch.stack(boxes)\n",
    "    \n",
    "\n",
    "    return r_boxes , torch.tensor(labels), torch.tensor(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Layer Detection\n",
    "def test():\n",
    "    model = resnetOneLayer().to(device)\n",
    "    model.load_state_dict(torch.load(\"./model/one_layer_t.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    map_th = 0.5\n",
    "\n",
    "    c_dir = os.getcwd()\n",
    "    typ = \"test\"\n",
    "    train_img_addr = c_dir + \"/\" + \"VOC_\" + typ + \"/JPEGImages\"\n",
    "    train_ann_addr = c_dir + \"/\" + \"VOC_\" + typ + \"/Annotations\"\n",
    "    train_images = os.listdir(train_img_addr)\n",
    "\n",
    "\n",
    "    det_boxes = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels = list()\n",
    "    true_difficulties = list()\n",
    "\n",
    "    ct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for each in train_images:\n",
    "            ct += 1\n",
    "            if ct % 50 == 0:  \n",
    "                APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties, map_th)\n",
    "                print(ct, APs, mAP)\n",
    "\n",
    "            img_name = train_img_addr + '/' + each \n",
    "            xml_file = train_ann_addr + '/' + each[:-3] + 'xml'\n",
    "            actual_boxes, act_boxes, act_labels, actual_difficulties = get_ground_truth(xml_file)\n",
    "            if len(actual_boxes) == 0:\n",
    "                continue\n",
    "            p_boxes, p_labels, p_scores = give_bounding_box(model, img_name, actual_boxes)\n",
    "            true_boxes.append(act_boxes)\n",
    "            true_labels.append(act_labels)\n",
    "            true_difficulties.append(actual_difficulties)\n",
    "            det_boxes.append(p_boxes)\n",
    "            det_labels.append(p_labels)\n",
    "            det_scores.append(p_scores)\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties, map_th)\n",
    "        print(ct, APs, mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Layer Detection\n",
    "def test():\n",
    "    model = resnetTwoLayer().to(device)\n",
    "    model.load_state_dict(torch.load(\"./model/two_layer_t.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    map_th = 0.5\n",
    "\n",
    "    c_dir = os.getcwd()\n",
    "    typ = \"test\"\n",
    "    train_img_addr = c_dir + \"/\" + \"VOC_\" + typ + \"/JPEGImages\"\n",
    "    train_ann_addr = c_dir + \"/\" + \"VOC_\" + typ + \"/Annotations\"\n",
    "    train_images = os.listdir(train_img_addr)\n",
    "\n",
    "\n",
    "    det_boxes = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels = list()\n",
    "    true_difficulties = list()\n",
    "\n",
    "    ct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for each in train_images:\n",
    "            ct += 1\n",
    "            if ct % 50 == 0:  \n",
    "                APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties, map_th)\n",
    "                print(ct, APs, mAP)\n",
    "\n",
    "            img_name = train_img_addr + '/' + each \n",
    "            xml_file = train_ann_addr + '/' + each[:-3] + 'xml'\n",
    "            actual_boxes, act_boxes, act_labels, actual_difficulties = get_ground_truth(xml_file)\n",
    "            if len(actual_boxes) == 0:\n",
    "                continue\n",
    "            p_boxes, p_labels, p_scores = give_bounding_box(model, img_name, actual_boxes)\n",
    "            true_boxes.append(act_boxes)\n",
    "            true_labels.append(act_labels)\n",
    "            true_difficulties.append(actual_difficulties)\n",
    "            det_boxes.append(p_boxes)\n",
    "            det_labels.append(p_labels)\n",
    "            det_scores.append(p_scores)\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties, map_th)\n",
    "        print(ct, APs, mAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Calculation script\n",
    "This script is taken from 3rd Party (a github repo utils) and accordlingly changed as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Label map\n",
    "voc_labels = ('aeroplane', 'bottle', 'chair')\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
    "\n",
    "def find_jaccard_overlap(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Find intersections\n",
    "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
    "\n",
    "    # Find areas of each box in both sets\n",
    "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
    "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
    "\n",
    "    # Find the union\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
    "\n",
    "    return intersection / union  # (n1, n2)\n",
    "\n",
    "\n",
    "\n",
    "def find_intersection(set_1, set_2):\n",
    "    \"\"\"\n",
    "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
    "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
    "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
    "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
    "    \"\"\"\n",
    "\n",
    "    # PyTorch auto-broadcasts singleton dimensions\n",
    "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
    "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
    "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
    "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
    "\n",
    "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties, map_th):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision (mAP) of detected objects.\n",
    "    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n",
    "    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n",
    "    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n",
    "    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n",
    "    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n",
    "    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n",
    "    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n",
    "    :return: list of average precisions for all classes, mean average precision (mAP)\n",
    "    \"\"\"\n",
    "    print(\"ewuhru\")\n",
    "\n",
    "    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n",
    "        true_labels) == len(\n",
    "        true_difficulties)  # these are all lists of tensors of the same length, i.e. number of images\n",
    "    n_classes = len(label_map)\n",
    "\n",
    "    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n",
    "    true_images = list()\n",
    "    for i in range(len(true_labels)):\n",
    "        true_images.extend([i] * true_labels[i].size(0))\n",
    "    true_images = torch.LongTensor(true_images).to(\n",
    "        device)  # (n_objects), n_objects is the total no. of objects across all images\n",
    "    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n",
    "    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n",
    "    true_difficulties = torch.cat(true_difficulties, dim=0)  # (n_objects)\n",
    "\n",
    "    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n",
    "\n",
    "    # Store all detections in a single continuous tensor while keeping track of the image it is from\n",
    "    det_images = list()\n",
    "    for i in range(len(det_labels)):\n",
    "        det_images.extend([i] * det_labels[i].size(0))\n",
    "    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n",
    "    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n",
    "    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n",
    "    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n",
    "\n",
    "    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n",
    "\n",
    "    # Calculate APs for each class (except background)\n",
    "    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n",
    "    for c in range(1, n_classes):\n",
    "        # Extract only objects with this class\n",
    "        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n",
    "        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n",
    "        true_class_difficulties = true_difficulties[true_labels == c]  # (n_class_objects)\n",
    "        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n",
    "\n",
    "        # Keep track of which true objects with this class have already been 'detected'\n",
    "        # So far, none\n",
    "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n",
    "            device)  # (n_class_objects)\n",
    "\n",
    "        # Extract only detections with this class\n",
    "        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n",
    "        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n",
    "        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n",
    "        n_class_detections = det_class_boxes.size(0)\n",
    "        if n_class_detections == 0:\n",
    "            continue\n",
    "\n",
    "        # Sort detections in decreasing order of confidence/scores\n",
    "        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n",
    "        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n",
    "        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n",
    "\n",
    "        # In the order of decreasing scores, check if true or false positive\n",
    "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
    "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
    "        for d in range(n_class_detections):\n",
    "            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n",
    "            this_image = det_class_images[d]  # (), scalar\n",
    "\n",
    "            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n",
    "            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n",
    "            object_difficulties = true_class_difficulties[true_class_images == this_image]  # (n_class_objects_in_img)\n",
    "            # If no such object in this image, then the detection is a false positive\n",
    "            if object_boxes.size(0) == 0:\n",
    "                false_positives[d] = 1\n",
    "                continue\n",
    "\n",
    "            # Find maximum overlap of this detection with objects in this image of this class\n",
    "            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n",
    "            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n",
    "\n",
    "            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n",
    "            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n",
    "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
    "            # We need 'original_ind' to update 'true_class_boxes_detected'\n",
    "\n",
    "            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n",
    "            if max_overlap.item() > map_th:\n",
    "                # If the object it matched with is 'difficult', ignore it\n",
    "                if object_difficulties[ind] == 0:\n",
    "                    # If this object has already not been detected, it's a true positive\n",
    "                    if true_class_boxes_detected[original_ind] == 0:\n",
    "                        true_positives[d] = 1\n",
    "                        true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n",
    "                    # Otherwise, it's a false positive (since this object is already accounted for)\n",
    "                    else:\n",
    "                        false_positives[d] = 1\n",
    "            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n",
    "            else:\n",
    "                false_positives[d] = 1\n",
    "\n",
    "        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n",
    "        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n",
    "        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n",
    "        cumul_precision = cumul_true_positives / (\n",
    "                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n",
    "        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n",
    "\n",
    "        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n",
    "        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = cumul_recall >= t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = cumul_precision[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.\n",
    "        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n",
    "\n",
    "    # Calculate Mean Average Precision (mAP)\n",
    "    mean_average_precision = average_precisions.mean().item()\n",
    "\n",
    "    # Keep class-wise average precisions in a dictionary\n",
    "    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n",
    "\n",
    "    return average_precisions, mean_average_precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
